<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KaleidoEye: A Large-scale Dataset and Benchmark for Slippage-Robust Gaze Tracking in HMDs</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.8/dist/chart.umd.min.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        primary: '#165DFF',        // 主色调：深蓝色
                        secondary: '#3B82F6',      // 辅助色：浅蓝色
                        accent: '#F97316',         // 强调色：橙色
                        dark: '#1E293B',           // 深色：深灰
                        light: '#F8FAFC',          // 浅色：接近白色
                        neutral: '#64748B',        // 中性色：中灰
                    },
                    fontFamily: {
                        inter: ['Inter', 'sans-serif'],
                    },
                }
            }
        }
    </script>

    <style type="text/tailwindcss">
        @layer utilities {
            .content-auto {
                content-visibility: auto;
            }
            .text-shadow {
                text-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
            .hover-scale {
                transition: transform 0.3s ease;
            }
            .hover-scale:hover {
                transform: scale(1.03);
            }
            .animate-fade-in {
                animation: fadeIn 0.8s ease-in-out;
            }
            .animate-slide-up {
                animation: slideUp 0.8s ease-out;
            }
            .pill-button {
                @apply px-6 py-2.5 bg-dark hover:bg-accent text-white rounded-full shadow-md hover:shadow-lg transition-all duration-300 flex items-center justify-center text-sm;
            }
            @keyframes fadeIn {
                from { opacity: 0; }
                to { opacity: 1; }
            }
            @keyframes slideUp {
                from { transform: translateY(20px); opacity: 0; }
                to { transform: translateY(0); opacity: 1; }
            }
            /* 新增表格单元格内边距样式 */
            .table-cell-padding {
                @apply px-4 py-2.5;  /* 调整内边距减少行高 */
            }
            /* 表格行高调整 */
            .table-row-height {
                height: 50%;  /* 原高度的75% */
            }
        }
    </style>
</head>
<body class="font-inter bg-light text-dark antialiased text-lg">
    <!-- 导航栏 -->
    <nav id="navbar" class="fixed w-full bg-white/90 backdrop-blur-md z-50 transition-all duration-300 shadow-sm">
        <div class="container mx-auto px-4 py-3 flex justify-between items-center">
            <div class="flex items-center space-x-2">
                <span class="text-2xl"><i class="fa fa-cube text-primary"></i></span>
                <span class="font-bold text-xl text-primary">KaleidoEye</span>
            </div>
            <div class="hidden md:flex space-x-8">
                <a href="#abstract" class="font-medium hover:text-primary transition-colors">Abstract</a>
                <a href="#dataset" class="font-medium hover:text-primary transition-colors">Dataset</a>
                <a href="#benchmark" class="font-medium hover:text-primary transition-colors">Benchmark</a>
                <a href="#results" class="font-medium hover:text-primary transition-colors">Results</a>
                <a href="#Details" class="font-medium hover:text-primary transition-colors">Details</a>
            </div>
            <div class="md:hidden">
                <button id="menu-toggle" class="text-dark focus:outline-none">
                    <i class="fa fa-bars text-primary text-xl"></i>
                </button>
            </div>
        </div>
        <!-- 移动端菜单 -->
        <div id="mobile-menu" class="hidden md:hidden bg-white shadow-lg animate-fade-in">
            <div class="container mx-auto px-4 py-2 flex flex-col space-y-3">
                <a href="#abstract" class="font-medium py-2 hover:text-primary transition-colors">Abstract</a>
                <a href="#dataset" class="font-medium py-2 hover:text-primary transition-colors">Dataset</a>
                <a href="#benchmark" class="font-medium py-2 hover:text-primary transition-colors">Benchmark</a>
                <a href="#results" class="font-medium py-2 hover:text-primary transition-colors">Results</a>
                <a href="#Details" class="font-medium py-2 hover:text-primary transition-colors">Details</a>
            </div>
        </div>
    </nav>

    <!-- 英雄区域 -->
    <header class="pt-24 pb-16 md:pt-32 md:pb-24 bg-gradient-to-br from-primary/5 to-light">
        <div class="container mx-auto px-4">
            <div class="max-w-7xl mx-auto text-center">
                <h1 class="text-[clamp(2rem,5vw,3.5rem)] font-bold leading-tight text-dark mb-6 animate-fade-in">
                    KaleidoEye: A Large-scale Dataset and Benchmark for Slippage-Robust Gaze Tracking in HMDs
                </h1>
                <p class="text-[clamp(1.1rem,2vw,1.3rem)] text-gray-600 mb-8 animate-slide-up">

                </p>
                <table align=center width=700px>
                    <tr>
                        <center>
                            <span style="font-size:20px">Yingxi Li<sup>1</sup></span> &nbsp;&nbsp;&nbsp;
                            <span style="font-size:20px">Xiaowei Bai<sup>2</sup></span> &nbsp;&nbsp;&nbsp;
                            <span style="font-size:20px">Liang Xie<sup>2</sup></span> &nbsp;&nbsp;&nbsp;
                            <span style="font-size:20px">Hao Su<sup>3</sup></span> &nbsp;
                            <span style="font-size:20px">Zhuoru Li<sup>4</sup></span> &nbsp;&nbsp;&nbsp;
                            <span style="font-size:20px">Feitian Zhang<sup>1</sup></span> &nbsp;&nbsp;&nbsp;
                            <span style="font-size:20px">Ye Yan<sup>1, 2</sup></span> &nbsp;
                            <span style="font-size:20px">Erwei Yin<sup>2</sup></span>
                        </center>
                    </tr>
                </table>

                <table align=center width=800px>
                    <tr>
                        <td align=center width=160px>
                            <center>
                                <span style="font-size:20px"><sup>1</sup>College of Engineering, Peking University</span> &nbsp;
                                <span style="font-size:20px"><sup>2</sup>Defense Innovation Institute, Academy of Military Sciences</span> &nbsp;
                                <span style="font-size:20px"><sup>3</sup>Zhengzhou University</span> &nbsp;
                                <span style="font-size:20px"><sup>4</sup>Tianjin University</span> &nbsp;
<!--                                <span style="font-size:20px"><sup>5</sup>Nankai University</span>-->
                            </center>
                        </td>
                    </tr>
                </table>

                <br>
                <div class="flex flex-wrap justify-center gap-4 animate-slide-up" style="animation-delay: 0.4s;">
                    <a href="https://github.com/Shirley0118/VIHand/tree/main/code" class="pill-button">
                        <i class="fa fa-github text-white mr-2"></i> Code
                    </a>
                    <a href="https://pan.quark.cn/s/ef5ca6eedc17" class="pill-button">
                        <i class="fa fa-database text-white mr-2"></i> Dataset
                    </a>
                    <a href="https://shirley0118.github.io/VIHand/SupplementaryMaterials.pdf" class="pill-button">
                        <i class="fa fa-file-pdf-o text-white mr-2"></i> Supplementary materials
                    </a>

                </div>
                <br>
                <br>


<!--<div class="video-container mt-8 animate-fade-in" style="width: 90%; margin: 0 auto;">-->
<!--    <video-->
<!--        controls-->
<!--        autoplay-->
<!--        loop-->
<!--        muted-->
<!--        class="w-full rounded-lg shadow-lg"-->
<!--        style="max-width: 100%; height: auto;">-->
<!--        <source src="https://shirley0118.github.io/VIHand/demo.mp4" type="video/mp4">-->
<!--        Your browser does not support the video tag.-->
<!--    </video>-->
<!--</div>-->

        <!-- 第一张图片容器 -->
<!--        <div style="width: 90%; margin: 0 auto;">-->
<!--            <img src="https://lency-xx.github.io/Kaleidoeye/Sample_imgs.png" alt="Dataset Overview"-->
<!--                 class="w-full h-auto rounded-lg mt-8">-->
<!--        </div>-->

        <!-- 第二张图片和文字容器 -->
        <div style="width: 90%; margin: 0 auto;">
            <img src="https://lency-xx.github.io/Kaleidoeye/kaleidoeye_dataset.jpg" alt="Dataset Overview"
                 class="w-full h-auto rounded-lg mt-8">
            <img src="https://lency-xx.github.io/Kaleidoeye/Sample_imgs.png" alt="Dataset Overview"
                 class="w-full h-auto rounded-lg mt-8">
            <p class="mt-6 text-lg text-gray-700 text-justify leading-relaxed animate-fade-in" style="animation-delay: 0.6s;">
                <strong>KaleidoEye</strong> is the first large-scale gaze-tracking dataset with quantifiable 6DoF slippage annotations (comprising 3DoF rotation and 3DoF translation between the headset and the user's head).
                It includes over 1.75 million samples from 30 participants, providing synchronized binocular eye images, 2D/3D eyeball annotations, gaze coordinates, and 6DoF HMD poses.
                By precisely varying headset poses using a robotic arm, we construct 11,130 slippage conditions, narrowing the domain gap between experimental settings and real-world usage.
            </p>
        </div>


<!--                <div style="width: 90%; margin-left: 5%;">-->
<!--                    <img src="https://lency-xx.github.io/Kaleidoeye/est_gaze_overview.jpg" alt="Dataset Overview" class="w-full h-auto rounded-lg mt-8">-->
<!--                    <p class="mt-6 text-lg text-gray-700 text-justify leading-relaxed animate-fade-in" style="animation-delay: 0.6s;">-->
<!--                        VIHand is the first large-scale glove-worn dataset for visual-inertial hand pose estimation,-->
<!--                        containing over 1.4 million synchronized RGB-D and IMU frames from 15 subjects.-->
<!--                        It provides accurate frame-level 3D joint annotations across complex gestures,-->
<!--                        enabling comprehensive research in HPE tasks, including multimodal fusion, cross-modal knowledge transfer and cross-modal data generation, etc.-->
<!--                    </p>-->
<!--                </div>-->
            </div>
        </div>
    </header>

    <!-- 摘要部分 -->
    <section id="abstract" class="py-12 scroll-mt-24">
        <div class="max-w-6xl mx-auto">
            <h2 class="text-[clamp(1.5rem,3vw,2.5rem)] font-bold mb-6 text-center">Abstract</h2>
            <p class="text-xl text-gray-700 leading-relaxed mb-6 text-justify">
           Egocentric gaze tracking is essential for immersive interaction in head-mounted displays (HMDs). However, existing end-to-end approaches primarily assume a fixed and tightly fitted headset placement, overlooking the inevitable HMD slippage under real-world conditions. Although some datasets with variable wearing postures have been proposed, the limited slippage amplitude and the unquantifiable postures cannot provide an impartial benchmark to evaluate slip-robust gaze-tracking algorithms. In this paper, we build KaleidoEye, the first large-scale gaze tracking dataset with quantified 6DoF slippage poses. It contains 1.75 million binocular images from 30 subjects, each annotated with 2D/3D eyeball annotations, and gaze coordinates. By covering 11,130 realistic slippage conditions, KaleidoEye captures extensive eye appearance distortions and gaze-space misalignments, providing a challenging benchmark for gaze-tracking algorithms. Furthermore, we propose EST-Gaze, a high-performance and slippage-robust gaze tracking model for resource-constrained devices. By leveraging edge-guided spatial transformations, it learns structural invariance in eye appearance under slip-induced distortions, effectively projecting distorted inputs onto a slippage-invariant subspace and simplifying the gaze mapping process. The evaluation results on KaleidoEye illustrate that EST-Gaze achieves an uncalibrated average error of 2.53$^\circ$ with minimal computational overhead, surpassing state-of-the-art methods.
            </p>
        </div>
    </section>


    <section id="collection" class="py-12 scroll-mt-24 bg-gray-50 rounded-2xl">
        <div class="max-w-6xl mx-auto">
            <h2 class="text-[clamp(1.5rem,3vw,2.5rem)] font-bold mb-10 text-center">Data Collection Overview</h2>
            <div class="mt-12 bg-white rounded-xl shadow-lg p-6">
                <p class="text-gray-700 mb-4 text-lg text-justify">Overview of the KaleidoEye data collection protocol. (a) A robotic arm controll HMD slippage while the subject's head remains fixed. (b) HoloLens and Pupil Core are jointly calibrated to obtain accurate gaze labels and 2D/3D annotations. (c) Slippage is executed under three types (RA-RC) and paired with diverse stimulus paradigms (PA-PC). (d) The dataset includes synchronized eye images, slippage postures, gaze targets, and dense annotations.

                </p>
                <div class="overflow-hidden rounded-lg">
                    <img src="https://lency-xx.github.io//Kaleidoeye/data_collection.jpg" alt="data collection overview" class="w-full h-auto object-cover rounded-lg">
                </div>
            </div>
        </div>
    </section>


<!--    <section id="dataset" class="py-12 scroll-mt-24 bg-gray-50 rounded-2xl">-->
<!--        <div class="max-w-6xl mx-auto">-->
<!--            <h2 class="text-[clamp(1.5rem,3vw,2.5rem)] font-bold mb-10 text-center">Dataset Samples</h2>-->
<!--            <div class="mt-12 bg-white rounded-xl shadow-lg p-6">-->
<!--                <p class="text-gray-700 mb-4 text-lg text-justify">Representative samples from the KaleidoEye dataset. Each row depicts a synchronized binocular frame, with the first four columns corresponding to the left eye and the latter four to the right eye. For each eye, we visualize (1) the raw NIR image, (2) pupil detection results with the fitted ellipse and center point, (3) a binary pupil segmentation mask, and (4) projected 3D eyeball, including the gaze vector (yellow), pupil ellipse (red), and iris contour (cyan).-->

<!--                </p>-->
<!--                <div class="overflow-hidden rounded-lg">-->
<!--                    <img src="https://lency-xx.github.io/Kaleidoeye/Sample_imgs.png" alt="dataset samples" class="w-full h-auto object-cover rounded-lg">-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->
<!--    </section>-->


    <!-- 方法部分 -->
    <section id="benchmark" class="py-12 scroll-mt-24 bg-gray-50 rounded-2xl">
        <div class="max-w-6xl mx-auto">
            <h2 class="text-[clamp(1.5rem,3vw,2.5rem)] font-bold mb-10 text-center">Benchmark</h2>
            <div class="mt-12 bg-white rounded-xl shadow-lg p-6">
                <p class="text-gray-700 mb-4 text-lg text-justify">Overview of the proposed EST-Gaze framework for slippage-robust gaze tracking. Given an eye image, we first extract a corresponding edge image using a multi-stage pipeline. Both the eye image and edge image are processed by separate encoders to produce feature maps $F_{\text{eye}}$ and $F_{\text{edge}}$, which are fused via a spatial attention module to generate edge weights that modulate the eye features. A pose-aware affine MLP estimates the transformation parameters $\theta$, which are used by a grid sampler to produce a spatially aligned image. The aligned image is then forwarded to an existing gaze tracking network for prediction. The entire framework is supervised by the predicted PoG and leverages both visual and geometric signals to mitigate HMD slippage.</p>
                <div class="overflow-hidden rounded-lg">
                    <img src="https://lency-xx.github.io/Kaleidoeye/est_gaze_overview.jpg" alt="Est_gaze_overview Architecture Diagram" class="w-full h-auto object-cover rounded-lg">
                </div>
            </div>
        </div>
    </section>

    <!-- 结果部分 -->
    <section id="results" class="py-12 scroll-mt-24">
        <div class="max-w-6xl mx-auto">
            <h2 class="text-[clamp(1.5rem,3vw,2.5rem)] font-bold mb-10 text-center">Results</h2>
            <div class="grid grid-cols-1 gap-6 mb-10">
                <div class="bg-white rounded-xl shadow-lg overflow-hidden hover-scale">
                    <div class="p-6">
                        <p class="text-gray-700 mb-4 text-lg text-justify"> Visual comparison between our proposed pupil center detection method (top row, green ellipses) and the Pupil Labs ground-truth annotations (bottom row, blue ellipses). Each group represents 3 representative samples from one of 6 subjects. Our method demonstrates strong alignment with the annotated pupil contours under various conditions including gaze shift, occlusion, and illumination changes.</p>
                        <p class="text-gray-700 mb-4">
                            <img src="https://lency-xx.github.io/Kaleidoeye/pupil.png" alt="Qualitative Result 1" class="rounded-lg w-full h-auto">
                        </p>
<!--                        <p class="text-gray-700 mb-4 text-lg text-justify"> Qualitative results validate that our VIFNet produces much better results than previous methods on VIHand.</p>-->
                        <div class="h-5">
                            <canvas id="benchmarkChart"></canvas>
                        </div>
                    </div>

                    <div class="p-6">
                        <p class="text-gray-700 mb-4 text-lg text-justify"> Qualitative results of Canny edge detection on adaptively extracted ROIs. Each block shows the original grayscale eye image (top) and the corresponding edge map (bottom). Our non-learning pipeline consistently captures key anatomical structures such as eyelid and pupil contours, enabling intuitive visualization of appearance shifts caused by HMD slippage.</p>
                        <p class="text-gray-700 mb-4">
                            <img src="https://lency-xx.github.io/Kaleidoeye/edge.png" alt="Qualitative Result 1" class="rounded-lg w-full h-auto">
                        </p>
<!--                        <p class="text-gray-700 mb-4 text-lg text-justify"> Qualitative results validate that our VIFNet produces much better results than previous methods on VIHand.</p>-->
                        <div class="h-5">
                            <canvas id="benchmarkChart1"></canvas>
                        </div>
                    </div>

                    <div class="p-6">
                        <p class="text-gray-700 mb-4 text-lg text-justify"> Qualitative comparison of slippage correction using the proposed EST module. The top row shows original eye images under slippage, exhibiting geometric distortions such as eyelid deformation. The middle row presents the aligned outputs after applying EST. The bottom row displays non-slippage reference samples for comparison. Green dashed lines indicate key structural regions, highlighting the improvement in spatial consistency after correction.</p>
                        <p class="text-gray-700 mb-4">
                            <img src="https://lency-xx.github.io/Kaleidoeye/qualitative_est.png" alt="Qualitative Result 1" class="rounded-lg w-full h-auto">
                        </p>
<!--                        <p class="text-gray-700 mb-4 text-lg text-justify"> Qualitative results validate that our VIFNet produces much better results than previous methods on VIHand.</p>-->
                        <div class="h-5">
                            <canvas id="benchmarkChart3"></canvas>
                        </div>
                    </div>


                    <div class="p-6">
                        <p class="text-gray-700 mb-4 text-lg text-justify"> Subject-wise angular error comparisons across six slippage sessions. Red points indicate the baseline NVGaze error, and green points show the results with EST-Gaze. Horizontal lines mark the mean error per method. EST consistently reduces prediction bias and suppresses high-error outliers under slippage conditions.</p>
                        <p class="text-gray-700 mb-4">
                            <img src="https://lency-xx.github.io/Kaleidoeye/nvgaze&estgaze-points.png" alt="Qualitative Result 1" class="rounded-lg w-full h-auto">
                        </p>
<!--                        <p class="text-gray-700 mb-4 text-lg text-justify"> Qualitative results validate that our VIFNet produces much better results than previous methods on VIHand.</p>-->
                        <div class="h-5">
                            <canvas id="benchmarkChart4"></canvas>
                        </div>
                    </div>

                </div>
            </div>
        </div>



    </section>


<!-- End Experiments -->

    <!-- Dataset Directory Structure -->
    <section id="Details" class="py-12 scroll-mt-24 bg-gray-100 rounded-2xl">
        <div class="max-w-6xl mx-auto px-4">
            <h2 class="text-[clamp(1.5rem,3vw,2.5rem)] font-bold mb-6 text-center">Dataset Directory Structure</h2>
            <div class="bg-white p-6 rounded-lg shadow-md overflow-x-auto">
                <pre class="text-lg font-mono leading-relaxed">
            ${ROOT}
            ├── multidata
            │   ├── subject1 ~ subject15           # 15 subjects
            │   │   ├── ROM01 ~ ROM15              # 15 gesture sequences
            │   │   │   ├── camera0 ~ camera4      # 5 camera views
            │   │   │   │   ├── rgb                # RGB sequences
            │   │   │   │   ├── depth              # Depth map sequences
            │   │   │   │   ├── bbox.json          # bounding box
            │   │   │   │   └── center.json        # Depth map center
            │   │   │   │── IMU.json               # IMU sequences
            │   │   │   └── camera_paras.json      # camera parameters
            │   │   │...
            │   │...
            ├── annotations
            │   ├── subject1 ~ subject15
            │   │   ├── ROM01 ~ ROM15
            │   │   │   ├── joint.json
            │   │   │   ├── mano.json
            │   │   │   └── mesh.json
            │   │   │...
            │   │...
                </pre>
            </div>
            <div class="note mt-6 bg-yellow-100 border-l-4 border-yellow-500 text-yellow-700 p-4 rounded-md">
                <p class="text-lg">
                    Note: The above directory structure is a representation of how the dataset is organized. Each directory and file serves a specific purpose in the dataset's organization.
                </p>
            </div>
        </div>
    </section>

    <!-- Dataset Details -->
    <section class="py-12 scroll-mt-24">
        <div class="max-w-7xl mx-auto px-4">
            <h2 class="text-[clamp(1.5rem,3vw,2.5rem)] font-bold mb-10 text-center">Dataset Details</h2>
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                <!-- 左侧列：两个表格 -->
                <div class="space-y-8">
                    <div class="bg-white rounded-xl shadow-lg overflow-hidden">
                        <div class="p-6 border-b border-gray-200">
                            <h3 class="text-xl font-bold text-center">Frame Distribution by Category</h3> <!-- 标题居中 -->
                        </div>
                        <div class="overflow-x-auto">
                            <table class="w-full table-row-height"> <!-- 应用行高调整 -->
                                <thead>
                                    <tr class="bg-gray-50">
                                        <th class="table-cell-padding text-left" style="width: 60%;">Category</th>
                                        <th class="table-cell-padding text-left" style="width: 40%;">Total Frames</th>
                                    </tr>
                                </thead>
                                <tbody class="bg-white divide-y divide-gray-200">
                                    <tr>
                                        <td class="table-cell-padding"><div class="text-lg font-bold text-gray-900">Train Set</div></td>
                                        <td class="table-cell-padding"><div class="text-lg font-bold gradient-bg inline-block px-2 py-0.5 rounded">1.06M</div></td>
                                    </tr>
                                    <!-- 删除了Validation Set行 -->
                                    <tr>
                                        <td class="table-cell-padding"><div class="text-lg font-bold text-gray-900">Test Set</div></td>
                                        <td class="table-cell-padding"><div class="text-lg font-bold bg-gray-200 text-gray-900 inline-block px-2 py-0.5 rounded">340K</div></td> <!-- 取消红色背景 -->
                                    </tr>
                                    <tr class="bg-gradient-to-r from-blue-500 to-purple-600">
                                        <td class="table-cell-padding"><div class="text-lg font-bold text-white">Total</div></td>
                                        <td class="table-cell-padding"><div class="text-lg font-bold text-white">1.4M</div></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>

                    <div class="bg-white rounded-xl shadow-lg overflow-hidden">
<!--                        <div class="p-6 border-b border-gray-200">-->
<!--                            <h3 class="text-xl font-bold text-center">Subject Information</h3> &lt;!&ndash; 标题居中 &ndash;&gt;-->
<!--                        </div>-->
                        <div class="overflow-x-auto">
                            <table class="w-full table-row-height"> <!-- 应用行高调整 -->
                                <thead>
                                    <tr class="bg-gray-50">
                                        <th class="table-cell-padding text-left" style="width: 60%;">Subject ID</th>
                                        <th class="table-cell-padding text-left" style="width: 40%;">Gender</th>
                                    </tr>
                                </thead>
                                <tbody class="bg-white divide-y divide-gray-200" id="subjectTable">
                                    <!-- 动态生成的内容 -->
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>

                <!-- 右侧列：长图片 -->
                <div>
                    <div class="bg-white rounded-xl shadow-lg overflow-hidden h-full flex flex-col">
                        <div class="p-6 border-b border-gray-200">
                            <h3 class="text-xl font-bold text-center">Data Sample</h3> <!-- 标题居中 -->
                        </div>
                        <div class="flex-grow flex items-center justify-center p-6">
                            <img src="https://shirley0118.github.io/VIHand/data_sample.jpg" alt="Data Sample" class="w-full h-auto rounded-lg shadow-md">
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

<!--     <div class="bg-white rounded-xl shadow-lg p-6 max-w-6xl mx-auto"> -->
<!--         <h3 class="text-xl font-semibold mb-4">Related Works</h3> -->
<!--         <ul class="space-y-4"> -->
<!--             <li class="flex items-start"> -->
<!--                 <div class="w-10 h-10 bg-primary/10 rounded-full flex items-center justify-center mr-4 mt-1 flex-shrink-0"> -->
<!--                     <i class="fa fa-file-text-o text-primary"></i> -->
<!--                 </div> -->
<!--                 <div> -->
<!--                     <a href="#" class="font-medium hover:text-primary transition-colors">A Dual-Branch Self-Boosting Framework for Self-Supervised 3D Hand Pose Estimation</a> -->
<!--                     <p class="text-gray-600 text-sm">Pengfei Ren et al.,TIP 2022</p> -->
<!--                 </div> -->
<!--             </li> -->
<!--             <li class="flex items-start"> -->
<!--                 <div class="w-10 h-10 bg-primary/10 rounded-full flex items-center justify-center mr-4 mt-1 flex-shrink-0"> -->
<!--                     <i class="fa fa-file-text-o text-primary"></i> -->
<!--                 </div> -->
<!--                 <div> -->
<!--                     <a href="#" class="font-medium hover:text-primary transition-colors">Mining multi-view information: a strong self-supervised framework for depth-based 3D hand pose and mesh estimation</a> -->
<!--                     <p class="text-gray-600 text-sm">Pengfei Ren et al.,CVPR 2022</p> -->
<!--                 </div> -->
<!--             </li> -->
<!--             <li class="flex items-start"> -->
<!--                 <div class="w-10 h-10 bg-primary/10 rounded-full flex items-center justify-center mr-4 mt-1 flex-shrink-0"> -->
<!--                     <i class="fa fa-file-text-o text-primary"></i> -->
<!--                 </div> -->
<!--                 <div> -->
<!--                     <a href="#" class="font-medium hover:text-primary transition-colors">Embodied Hands: Modeling and Capturing Hands and Bodies Together</a> -->
<!--                     <p class="text-gray-600 text-sm">Javier Romero et al.,TOG 2017</p> -->
<!--                 </div> -->
<!--             </li> -->
<!--         </ul> -->
<!--     </div> -->

<!--     Acknowledgments部分 -->
    <section id="acknowledgments" class="py-12 scroll-mt-24">
        <div class="max-w-4xl mx-auto">
            <h2 class="text-[clamp(1.5rem,3vw,2.5rem)] font-bold mb-8 text-center">Acknowledgments</h2>
            <p class="text-gray-700 text-justify">
                This work was supported in part by the National Natural Science Foundation of China (Nos. 62332019, 62406039),
the National Key R&D Program of China (Nos. 2023YFF1203900, 2023YFF1203903),
the China Postdoctoral Science Foundation (Nos. 2023TQ0039, 2024M750257, GZC20230320),
and the Beijing Nova Program (No. 20240484513)
            </p>
        </div>
    </section>

    <script>
        // 导航栏滚动效果
        window.addEventListener('scroll', function() {
            const navbar = document.getElementById('navbar');
            if (window.scrollY > 50) {
                navbar.classList.add('py-2', 'shadow-md');
                navbar.classList.remove('py-3', 'shadow-sm');
            } else {
                navbar.classList.add('py-3', 'shadow-sm');
                navbar.classList.remove('py-2', 'shadow-md');
            }
        });

        // 移动端菜单切换
        document.getElementById('menu-toggle').addEventListener('click', function() {
            const mobileMenu = document.getElementById('mobile-menu');
            mobileMenu.classList.toggle('hidden');
        });

        // 平滑滚动
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();

                const targetId = this.getAttribute('href');
                const targetElement = document.querySelector(targetId);

                if (targetElement) {
                    window.scrollTo({
                        top: targetElement.offsetTop - 80,
                        behavior: 'smooth'
                    });

                    // 关闭移动端菜单
                    document.getElementById('mobile-menu').classList.add('hidden');
                }
            });
        });

        // 生成受试者表格内容
        document.addEventListener('DOMContentLoaded', () => {
            const table = document.getElementById('subjectTable');
            const subjects = [
                { id: 'subject1', gender: 'Male' },
                { id: 'subject2', gender: 'Female' },
                { id: 'subject3', gender: 'Male' },
                { id: 'subject4', gender: 'Male' },
                { id: 'subject5', gender: 'Female' },
                { id: 'subject6', gender: 'Female' },
                { id: 'subject7', gender: 'Male' },
                { id: 'subject8', gender: 'Male' },
                { id: 'subject9', gender: 'Female' },
                { id: 'subject10', gender: 'Male' },
                { id: 'subject11', gender: 'Female' },
                { id: 'subject12', gender: 'Male' },
                { id: 'subject13', gender: 'Female' },
                { id: 'subject14', gender: 'Male' },
                { id: 'subject15', gender: 'Female' }
            ];

            subjects.forEach(subject => {
                const row = document.createElement('tr');
                row.className = 'h-8';  /* 固定行高 */

                const idCell = document.createElement('td');
                idCell.className = 'table-cell-padding whitespace-nowrap';
                idCell.innerHTML = `<div class="text-sm font-medium ${subject.gender === 'Female' ? 'text-pink-900' : 'text-blue-900'}">${subject.id}</div>`;

                const genderCell = document.createElement('td');
                genderCell.className = 'table-cell-padding whitespace-nowrap';
                genderCell.innerHTML = `
                    <div class="flex items-center">
                        <span class="w-2.5 h-2.5 rounded-full mr-1.5 ${subject.gender === 'Female' ? 'bg-pink-600' : 'bg-blue-600'}"></span>
                        <span class="text-sm font-medium ${subject.gender === 'Female' ? 'text-pink-900' : 'text-blue-900'}">${subject.gender}</span>
                    </div>
                `;

                row.appendChild(idCell);
                row.appendChild(genderCell);
                table.appendChild(row);
            });
        });
    </script>
</body>
</html>
